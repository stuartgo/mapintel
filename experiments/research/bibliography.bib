
@article{berry1995,
  title = {Using Linear Algebra for Intelligent Information Retrieval},
  author = {Berry, Michael W. and Dumais, Susan T. and O'Brien, Gavin W.},
  year = {1995},
  volume = {37},
  pages = {573--595},
  publisher = {{SIAM}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\RR58X3XG\\Berry et al. - 1995 - Using linear algebra for intelligent information r.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\K4VVY48V\\1037127.html},
  journal = {SIAM review},
  keywords = {Embeddings,NLP},
  number = {4}
}

@article{blei2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  volume = {3},
  pages = {993--1022},
  publisher = {{JMLR. org}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\PD9SIEF8\\Blei et al. - 2003 - Latent dirichlet allocation.pdf},
  journal = {the Journal of machine Learning research},
  keywords = {Embeddings,NLP}
}

@article{conneau2018,
  title = {Senteval: {{An}} Evaluation Toolkit for Universal Sentence Representations},
  shorttitle = {Senteval},
  author = {Conneau, Alexis and Kiela, Douwe},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1803.05449},
  eprinttype = {arxiv},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\68VAVMDP\\Conneau and Kiela - 2018 - Senteval An evaluation toolkit for universal sent.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\QISHR4LV\\1803.html},
  journal = {arXiv preprint arXiv:1803.05449},
  keywords = {Embeddings}
}

@article{conneau2019,
  title = {Unsupervised Cross-Lingual Representation Learning at Scale},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  archiveprefix = {arXiv},
  eprint = {1911.02116},
  eprinttype = {arxiv},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\FD93ZRR6\\Conneau et al. - 2019 - Unsupervised cross-lingual representation learning.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\GJ3JEUBN\\1911.html},
  journal = {arXiv preprint arXiv:1911.02116},
  keywords = {BERT,Embeddings,NLP}
}

@article{deerwester1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  year = {1990},
  volume = {41},
  pages = {391--407},
  publisher = {{Wiley Online Library}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\YSPY2TKF\\Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\3QVT4UJ4\\(SICI)1097-4571(199009)416391AID-ASI13.0.html},
  journal = {Journal of the American society for information science},
  keywords = {Embeddings,NLP},
  number = {6}
}

@inproceedings{dumais1988,
  title = {Using Latent Semantic Analysis to Improve Access to Textual Information},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems},
  author = {Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Deerwester, Scott and Harshman, Richard},
  year = {1988},
  pages = {281--285},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\8XIGQ3EM\\Dumais et al. - 1988 - Using latent semantic analysis to improve access t.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\VX9F8AVP\\57167.html},
  keywords = {Embeddings,NLP}
}

@article{harris1954,
  title = {Distributional Structure},
  author = {Harris, Zellig S.},
  year = {1954},
  volume = {10},
  pages = {146--162},
  publisher = {{Taylor \& Francis}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\99EYM5II\\Harris - 1954 - Distributional structure.pdf},
  journal = {Word},
  keywords = {Embeddings,NLP},
  number = {2-3}
}

@article{henriques2012,
  title = {Exploratory Geospatial Data Analysis Using the {{GeoSOM}} Suite},
  author = {Henriques, Roberto and Bacao, Fernando and Lobo, Victor},
  year = {2012},
  volume = {36},
  pages = {218--232},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\6P28EDC6\\S0198971511001141.html},
  journal = {Computers, Environment and Urban Systems},
  keywords = {SOM},
  number = {3}
}

@inproceedings{hofmann1999,
  title = {Probabilistic Latent Semantic Indexing},
  booktitle = {Proceedings of the 22nd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Hofmann, Thomas},
  year = {1999},
  pages = {50--57},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\YY2CTWDC\\Hofmann - 1999 - Probabilistic latent semantic indexing.pdf},
  keywords = {Embeddings,NLP}
}

@inproceedings{hu2008,
  title = {Collaborative Filtering for Implicit Feedback Datasets},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
  year = {2008},
  pages = {263--272},
  publisher = {{Ieee}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\2MSUVD8I\\4781121.html},
  keywords = {Recommender}
}

@article{ji2019,
  title = {Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection},
  shorttitle = {Visual Exploration of Neural Document Embedding in Information Retrieval},
  author = {Ji, Xiaonan and Shen, Han-Wei and Ritter, Alan and Machiraju, Raghu and Yen, Po-Yin},
  year = {2019},
  volume = {25},
  pages = {2181--2192},
  publisher = {{IEEE}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\CTMY3ZKR\\8667702.html},
  journal = {IEEE transactions on visualization and computer graphics},
  keywords = {Visual Analytics},
  number = {6}
}

@article{jones1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Jones, Karen Sparck},
  year = {1972},
  publisher = {{MCB UP Ltd}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\NDWF8NF5\\html.html},
  journal = {Journal of documentation},
  keywords = {Embeddings,NLP}
}

@article{kaski1998,
  title = {{{WEBSOM}}\textendash Self-Organizing Maps of Document Collections},
  author = {Kaski, Samuel and Honkela, Timo and Lagus, Krista and Kohonen, Teuvo},
  year = {1998},
  volume = {21},
  pages = {101--117},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\HDSTBCR7\\S0925231298000393.html},
  journal = {Neurocomputing},
  keywords = {SOM},
  number = {1-3}
}

@article{kohonen1982,
  title = {Self-Organized Formation of Topologically Correct Feature Maps},
  author = {Kohonen, Teuvo},
  year = {1982},
  volume = {43},
  pages = {59--69},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\KLCVASJ3\\BF00337288.html},
  journal = {Biological cybernetics},
  keywords = {SOM},
  number = {1}
}

@incollection{kohonen2001,
  title = {Software {{Tools}} for {{SOM}}},
  booktitle = {Self-Organizing Maps},
  author = {Kohonen, Teuvo},
  year = {2001},
  pages = {311--328},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\WRXYPH5N\\978-3-642-56927-2_8.html},
  keywords = {SOM}
}

@article{kohonen2013,
  title = {Essentials of the Self-Organizing Map},
  author = {Kohonen, Teuvo},
  year = {2013},
  volume = {37},
  pages = {52--65},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\5TE6QERW\\S0893608012002596.html},
  journal = {Neural networks},
  keywords = {SOM}
}

@inproceedings{lafia2019,
  title = {Enabling the {{Discovery}} of {{Thematically Related Research Objects}} with {{Systematic Spatializations}}},
  booktitle = {14th {{International Conference}} on {{Spatial Information Theory}} ({{COSIT}} 2019)},
  author = {Lafia, Sara and Last, Christina and Kuhn, Werner},
  year = {2019},
  publisher = {{Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\IZPH57LI\\Lafia et al. - 2019 - Enabling the Discovery of Thematically Related Res.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\9B4J3VJQ\\11110.html},
  keywords = {Visual Analytics}
}

@article{lagus1999,
  title = {Keyword Selection Method for Characterizing Text Document Maps},
  author = {Lagus, Krista and Kaski, Samuel},
  year = {1999},
  publisher = {{IET}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\9VRME94A\\Lagus and Kaski - 1999 - Keyword selection method for characterizing text d.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\FHXXA24S\\cp_19991137.html}
}

@inproceedings{le2014,
  title = {Distributed Representations of Sentences and Documents},
  booktitle = {International Conference on Machine Learning},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  pages = {1188--1196},
  publisher = {{PMLR}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\4JSL9BF7\\Le and Mikolov - 2014 - Distributed representations of sentences and docum.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\XBA6NESB\\le14.html},
  keywords = {Embeddings,NLP}
}

@article{mcinnes2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/QBY4NTED/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/home/dsilva/Zotero/storage/VNZZEC3A/1802.html},
  journal = {arXiv:1802.03426 [cs, stat]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@misc{moosavi2014,
  title = {{{SOMPY}}: {{A Python Library}} for {{Self Organizing Map}} ({{SOM}})},
  author = {Moosavi, Vahid and Packmann, S and Vall{\'e}s, Ivan},
  year = {2014},
  month = feb,
  abstract = {A Python Library for Self Organizing Map (SOM). Contribute to sevamoo/SOMPY development by creating an account on GitHub.},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  keywords = {SOM}
}

@article{papadimitriou2000,
  title = {Latent Semantic Indexing: {{A}} Probabilistic Analysis},
  shorttitle = {Latent Semantic Indexing},
  author = {Papadimitriou, Christos H. and Raghavan, Prabhakar and Tamaki, Hisao and Vempala, Santosh},
  year = {2000},
  volume = {61},
  pages = {217--235},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\LXNTN7QA\\S0022000000917112.html},
  journal = {Journal of Computer and System Sciences},
  keywords = {Embeddings,NLP},
  number = {2}
}

@book{schutze2008,
  title = {Introduction to Information Retrieval},
  author = {Sch{\"u}tze, Hinrich and Manning, Christopher D. and Raghavan, Prabhakar},
  year = {2008},
  volume = {39},
  publisher = {{Cambridge University Press Cambridge}},
  keywords = {IR}
}

@incollection{ultsch1993,
  title = {Self-Organizing Neural Networks for Visualisation and Classification},
  booktitle = {Information and Classification},
  author = {Ultsch, Alfred},
  year = {1993},
  pages = {307--313},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\JGFQZ38C\\978-3-642-50974-2_31.html},
  keywords = {SOM,U-Matrix}
}

@article{vandermaaten2008,
  title = {Visualizing Data Using T-{{SNE}}.},
  author = {{Van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  volume = {9},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\SJKFEZZM\\Van der Maaten and Hinton - 2008 - Visualizing data using t-SNE..pdf},
  journal = {Journal of machine learning research},
  keywords = {t-SNE},
  number = {11}
}

@book{murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  publisher = {{MIT press}},
}

@article{mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/XRZF2YA4/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/dsilva/Zotero/storage/4JYJF748/1301.html},
  journal = {arXiv:1301.3781 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/8ULXIGGK/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/dsilva/Zotero/storage/LUEA3YPW/1706.html},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{devlin2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/ADUJ2UGJ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/dsilva/Zotero/storage/YYX753IC/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/CSWIAXV5/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/home/dsilva/Zotero/storage/PIPH6M4G/1908.html},
  journal = {arXiv:1908.10084 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{lafia2021a,
  title = {Mapping Research Topics at Multiple Levels of Detail},
  author = {Lafia, Sara and Kuhn, Werner and Caylor, Kelly and Hemphill, Libby},
  year = {2021},
  month = mar,
  volume = {2},
  pages = {100210},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100210},
  abstract = {The institutional review of interdisciplinary bodies of research lacks methods to systematically produce higher-level abstractions. Abstraction methods, like the ``distant reading'' of corpora, are increasingly important for knowledge discovery in the sciences and humanities. We demonstrate how abstraction methods complement the metrics on which research reviews currently rely. We model cross-disciplinary topics of research publications and projects emerging at multiple levels of detail in the context of an institutional review of the Earth Research Institute (ERI) at the University of California at Santa Barbara. From these, we design science maps that reveal the latent thematic structure of ERI's interdisciplinary research and enable reviewers to ``read'' a body of research at multiple levels of detail. We find that our approach provides decision support and reveals trends that strengthen the institutional review process by exposing regions of thematic expertise, distributions and clusters of work, and the evolution of these aspects.},
  journal = {Patterns},
  keywords = {data discovery,decision support,institutional review,knowledge representation,science mapping,spatialization,topic modeling},
  language = {en},
  number = {3}
}


@article{lee1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {788--791},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/44565},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
  copyright = {1999 Macmillan Magazines Ltd.},
  file = {/home/dsilva/Zotero/storage/DFW74B2R/Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf;/home/dsilva/Zotero/storage/AYLQDFU8/44565.html},
  journal = {Nature},
  language = {en},
  number = {6755}
}

@techreport{calof2017,
  title = {Competitive {{Intelligence}}: {{A}} 10-Year {{Global Development}}},
  author = {Calof, Jonathan and Sewdass, Nisha and Arcos, Rub{\'e}n},
  year = {2017},
  institution = {{Competitive Intelligence Foundation}}
}

@article{marin2004,
  title = {Dissemination of {{Competitive Intelligence}}},
  author = {Marin, Jane and Poulter, Alan},
  year = {2004},
  month = apr,
  volume = {30},
  pages = {165--180},
  publisher = {{SAGE Publications Ltd}},
  issn = {0165-5515},
  doi = {10.1177/0165551504042806},
  abstract = {The paper argues that competitive intelligence is a vital function and attempts to study how it is distributed, especially by technologies, within organizations. Related topics, the sources of competitive intelligence, and who distributes and receives competitive intelligence, are also addressed. A literature-based study is extended by a quantitative survey of members of the Society of Competitive Information Professionals (SCIP) and email interviews with a self-chosen sample of respondees. The paper concludes that the distribution of competitive intelligence can be aided by technology but to be effective must be primarily `person-focused'. Competitive intelligence itself needs to be seen as a form of knowledge management rather than an information provision function. This has implications for sources and for the professional roots of those who provide competitive intelligence. Evaluation of competitive information provision is seen as a next step for research.},
  file = {/home/dsilva/Zotero/storage/PM6YEY32/Marin and Poulter - 2004 - Dissemination of Competitive Intelligence.pdf},
  journal = {Journal of Information Science},
  keywords = {competitive intelligence,competitive intelligence professionals,data analysis,evaluation,information dissemination,knowledge management,library and information professionals,personal communication,skills,technology use,usage,users},
  language = {en},
  number = {2}
}

@article{brod1999,
  title={Competitive Intelligence: Harvesting information to compete and market intelligently},
  author={Brod, Susan},
  journal={Camares Communications, New York, NY},
  year={1999}
}

@inproceedings{dey2011,
  title = {Acquiring Competitive Intelligence from Social Media},
  booktitle = {Proceedings of the 2011 {{Joint Workshop}} on {{Multilingual OCR}} and {{Analytics}} for {{Noisy Unstructured Text Data}}},
  author = {Dey, Lipika and Haque, Sk. Mirajul and Khurdiya, Arpit and Shroff, Gautam},
  year = {2011},
  month = sep,
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2034617.2034621},
  abstract = {Competitive intelligence is the art of defining, gathering and analyzing intelligence about competitor's products, promotions, sales etc. from external sources. The Web comes across as an important source for gathering competitive intelligence. News, blogs, as well as social media not only provide competitors information but also provide direct comparison of customer behaviors with respect to different verticals among competing organizations. This paper discusses methodologies to obtain competitive intelligence from different types of web resources including social media using a wide array of text mining techniques. It provides some results from case-studies to show how the gathered information can be integrated with structured data and used to explain business facts and thereby adopted for future decision making.},
  isbn = {978-1-4503-0685-0},
  keywords = {business intelligence from social media,competitive intelligence,content analysis,text mining},
  series = {{{MOCR}}\_{{AND}} '11}
}

@article{malkov2018,
  title = {Efficient and Robust Approximate Nearest Neighbor Search Using {{Hierarchical Navigable Small World}} Graphs},
  author = {Malkov, Yu A. and Yashunin, D. A.},
  year = {2018},
  month = aug,
  abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
  archiveprefix = {arXiv},
  eprint = {1603.09320},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/X2MQEFIW/Malkov and Yashunin - 2018 - Efficient and robust approximate nearest neighbor .pdf;/home/dsilva/Zotero/storage/P5KKSX5L/1603.html},
  journal = {arXiv:1603.09320 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Data Structures and Algorithms,Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  primaryclass = {cs}
}

@article{bajaj2018,
  title = {{{MS MARCO}}: {{A Human Generated MAchine Reading COmprehension Dataset}}},
  shorttitle = {{{MS MARCO}}},
  author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
  year = {2018},
  month = oct,
  abstract = {We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
  archiveprefix = {arXiv},
  eprint = {1611.09268},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/GWRZSQ6M/Bajaj et al. - 2018 - MS MARCO A Human Generated MAchine Reading COmpre.pdf;/home/dsilva/Zotero/storage/6KBI773E/1611.html},
  journal = {arXiv:1611.09268 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  primaryclass = {cs}
}

@article{nogueira2020a,
  title = {Passage {{Re}}-Ranking with {{BERT}}},
  author = {Nogueira, Rodrigo and Cho, Kyunghyun},
  year = {2020},
  month = apr,
  abstract = {Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27\% (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert},
  archiveprefix = {arXiv},
  eprint = {1901.04085},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/VIB9A7BZ/Nogueira and Cho - 2020 - Passage Re-ranking with BERT.pdf;/home/dsilva/Zotero/storage/HRA2DF28/1901.html},
  journal = {arXiv:1901.04085 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{kratzwald2019,
  title = {{{RankQA}}: {{Neural Question Answering}} with {{Answer Re}}-{{Ranking}}},
  shorttitle = {{{RankQA}}},
  author = {Kratzwald, Bernhard and Eigenmann, Anna and Feuerriegel, Stefan},
  year = {2019},
  month = aug,
  abstract = {The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text passages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused. In contrast, this work proposes RankQA: RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking. The re-ranking leverages different features that are directly extracted from the QA pipeline, i.e., a combination of retrieval and comprehension features. While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outperforms more complex QA systems by a significant margin: in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.},
  archiveprefix = {arXiv},
  eprint = {1906.03008},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/8U4QZV24/Kratzwald et al. - 2019 - RankQA Neural Question Answering with Answer Re-R.pdf;/home/dsilva/Zotero/storage/IVMHY3X7/1906.html},
  journal = {arXiv:1906.03008 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@misc{grootendorst2020,
  author       = {Maarten Grootendorst},
  title        = {BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.},
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.7.0},
  doi          = {10.5281/zenodo.4381785},
  url          = {https://doi.org/10.5281/zenodo.4381785}
}

@article{angelov2020,
  title = {{{Top2Vec}}: {{Distributed Representations}} of {{Topics}}},
  shorttitle = {{{Top2Vec}}},
  author = {Angelov, Dimo},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.09470 [cs, stat]},
  eprint = {2008.09470},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present \$\textbackslash texttt\{top2vec\}\$, which leverages joint document and word semantic embedding to find \$\textbackslash textit\{topic vectors\}\$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that \$\textbackslash texttt\{top2vec\}\$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/dsilva/Zotero/storage/KY7BYTSB/Angelov - 2020 - Top2Vec Distributed Representations of Topics.pdf;/home/dsilva/Zotero/storage/6F8XGGQM/2008.html}
}

@article{brown2020,
  title = {Language {{Models}} Are {{Few}}-{{Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  journal = {arXiv:2005.14165 [cs]},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dsilva/Zotero/storage/5333CTYJ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/dsilva/Zotero/storage/34YGARGS/2005.html}
}

@article{mcinnes2017,
  title={hdbscan: Hierarchical density based clustering},
  author={McInnes, Leland and Healy, John and Astels, Steve},
  journal={The Journal of Open Source Software},
  volume={2},
  number={11},
  pages={205},
  year={2017}
}

@article{humeau2019,
  title = {Poly-Encoders: {{Transformer Architectures}} and {{Pre}}-Training {{Strategies}} for {{Fast}} and {{Accurate Multi}}-Sentence {{Scoring}}},
  shorttitle = {Poly-Encoders},
  author = {Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
  year = {2019},
  month = apr,
  abstract = {The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on three existing tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.},
  language = {en},
  file = {/home/dsilva/Zotero/storage/BDB6RVYM/Humeau et al. - 2019 - Poly-encoders Transformer Architectures and Pre-t.pdf;/home/dsilva/Zotero/storage/LR74EGSM/1905.html}
}

@article{erdelez2020,
  title = {Information Encountering Re-Encountered: {{A}} Conceptual Re-Examination of Serendipity in the Context of Information Acquisition},
  shorttitle = {Information Encountering Re-Encountered},
  author = {Erdelez, Sanda and Makri, Stephann},
  year = {2020},
  month = jan,
  journal = {Journal of Documentation},
  volume = {76},
  number = {3},
  pages = {731--751},
  publisher = {{Emerald Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/JD-08-2019-0151},
  abstract = {Purpose In order to understand the totality, diversity and richness of human information behavior, increasing research attention has been paid to examining serendipity in the context of information acquisition. However, several issues have arisen as this research subfield has tried to find its feet; we have used different, inconsistent terminology to define this phenomenon (e.g. information encountering, accidental information discovery, incidental information acquisition), the scope of the phenomenon has not been clearly defined and its nature was not fully understood or fleshed-out. Design/methodology/approach In this paper, information encountering (IE) was proposed as the preferred term for serendipity in the context of information acquisition. Findings A reconceptualized definition and scope of IE was presented, a temporal model of IE and a refined model of IE that integrates the IE process with contextual factors and extends previous models of IE to include additional information acquisition activities pre- and postencounter. Originality/value By providing a more precise definition, clearer scope and richer theoretical description of the nature of IE, there was hope to make the phenomenon of serendipity in the context of information acquisition more accessible, encouraging future research consistency and thereby promoting deeper, more unified theoretical development.},
  keywords = {Information behavior,Information encountering,Information seeking,Models,Passive information acquisition,Serendipity},
  file = {/home/dsilva/Zotero/storage/RECU5KIA/Erdelez and Makri - 2020 - Information encountering re-encountered A concept.pdf;/home/dsilva/Zotero/storage/4S2HNTZ4/html.html}
}

@article{sampathila2020,
  title = {Computational Approach for Content-Based Image Retrieval of {{K}}-Similar Images from Brain {{MR}} Image Database},
  author = {Sampathila, Niranjana and Pavithra and Martis, Roshan Joy},
  year = {2020},
  journal = {Expert Systems},
  volume = {n/a},
  number = {n/a},
  pages = {e12652},
  issn = {1468-0394},
  doi = {10.1111/exsy.12652},
  abstract = {Content-based medical image retrieval (CBMIR) is a mechanism to handle a huge quantity of image data generated in various medical imaging modalities. In recent years, due to the evolution of computer vision and digital imaging modalities, a large number of medical images are generated. Consequently, the task of retrieving medical images from a large image database becomes more tedious due to variation in the size and shape of the images. Hence, it is necessary to design an appropriate system for medical image retrieval. In this paper a methodology for CBMIR using features of an image such as colour, shape, and texture is proposed to represent and retrieve the images from a large database that are relevant to a given query image. This methodology is evaluated for the application of retrieving the brain MRI images of different planes (coronal, sagittal, and transverse) from a dataset of normal and demented subjects. The features are determined in terms of Grey level co-occurrence based Haralik's features and histogram based cumulative distribution function (CDF). The image retrieval mechanism is designed using the K-Nearest Neighbour algorithm by finding the minimum distance between query and database images. The performance parameters such as precision and recall are calculated. The average accuracy of 95.5\% are obtained. The results provided ensures the capability to use it as assistive framework for radiologists in radiology image retrieval and classification.},
  language = {en},
  keywords = {CBMIR,K nearest neighbour,medical image,MRI,query image},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12652},
  file = {/home/dsilva/Zotero/storage/VBQURPXA/exsy.html}
}

@article{esteva2020,
  title = {{{CO}}-{{Search}}: {{COVID}}-19 {{Information Retrieval}} with {{Semantic Search}}, {{Question Answering}}, and {{Abstractive Summarization}}},
  shorttitle = {{{CO}}-{{Search}}},
  author = {Esteva, Andre and Kale, Anuprit and Paulus, Romain and Hashimoto, Kazuma and Yin, Wenpeng and Radev, Dragomir and Socher, Richard},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.09595 [cs]},
  eprint = {2006.09595},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The COVID-19 global pandemic has resulted in international efforts to understand, track, and mitigate the disease, yielding a significant corpus of COVID-19 and SARS-CoV-2-related publications across scientific disciplines. As of May 2020, 128,000 coronavirus-related publications have been collected through the COVID-19 Open Research Dataset Challenge. Here we present CO-Search, a retriever-ranker semantic search engine designed to handle complex queries over the COVID-19 literature, potentially aiding overburdened health workers in finding scientific answers during a time of crisis. The retriever is built from a Siamese-BERT encoder that is linearly composed with a TF-IDF vectorizer, and reciprocal-rank fused with a BM25 vectorizer. The ranker is composed of a multi-hop question-answering module, that together with a multi-paragraph abstractive summarizer adjust retriever scores. To account for the domain-specific and relatively limited dataset, we generate a bipartite graph of document paragraphs and citations, creating 1.3 million (citation title, paragraph) tuples for training the encoder. We evaluate our system on the data of the TREC-COVID information retrieval challenge. CO-Search obtains top performance on the datasets of the first and second rounds, across several key metrics: normalized discounted cumulative gain, precision, mean average precision, and binary preference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/dsilva/Zotero/storage/475Z4PAZ/Esteva et al. - 2020 - CO-Search COVID-19 Information Retrieval with Sem.pdf;/home/dsilva/Zotero/storage/FTIZXSS6/2006.html}
}

@article{madureira2021,
  title = {Competitive Intelligence: {{A}} Unified View and Modular Definition},
  shorttitle = {Competitive Intelligence},
  author = {Madureira, Lu{\'i}s and Popovi{\v c}, Ale{\v s} and Castelli, Mauro},
  year = {2021},
  month = dec,
  journal = {Technological Forecasting and Social Change},
  volume = {173},
  pages = {121086},
  issn = {0040-1625},
  doi = {10.1016/j.techfore.2021.121086},
  abstract = {The study aimed to identify the core defining dimensions and descriptors of Competitive Intelligence (CI) to provide a unified view and approach. The authors used a mixed-methods approach to derive meta-inferences from the sequential integration of quantitative and qualitative methods. Five defining core dimensions and one hundred descriptors, twenty for each dimension, were identified. The integrated dimensions provide a consistent definition and understanding of CI. More precise definitions result from cascading down the meaning of the dimensions into the descriptors, from complex to simple concepts. Grouping the descriptors allows for more concise explanations without loss of precision. A unified definition establishes the body of knowledge of the discipline and advances business and science. It also supports the establishment of the CI profession and professional identity and serves as a guide for the effective establishment of the CI function. The development of CI theory has a significant impact on CI culture and may enable the society to address one of the greatest current challenges, information overload. CI education can only thrive if a clear definition and understanding is possible.},
  language = {en},
  keywords = {Definition,Descriptors,Dimensions,Integration,Mixed methods,Theory building},
  file = {/home/dsilva/Zotero/storage/J3S36A3S/S0040162521005199.html}
}

